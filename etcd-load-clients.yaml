apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd-load-clients
  namespace: csi-mount-test
  labels:
    app: etcd-load-client
spec:
  replicas: 5
  selector:
    matchLabels:
      app: etcd-load-client
  template:
    metadata:
      labels:
        app: etcd-load-client
    spec:
      containers:
      - name: etcd-client
        image: artifactory.netskope.io/pe-docker/etcd-csi-test:v0.0.1
        securityContext:
          runAsUser: 0
          runAsGroup: 0
          allowPrivilegeEscalation: false
        command:
        - /bin/sh
        - -c
        - |
          # Wait for etcd to be ready
          echo "Waiting for etcd cluster to be ready..."
          while ! etcdctl --endpoints=http://etcd-cluster-0.etcd-headless:2379 endpoint health 2>/dev/null && \
                ! etcdctl --endpoints=http://etcd-cluster-1.etcd-headless:2379 endpoint health 2>/dev/null && \
                ! etcdctl --endpoints=http://etcd-cluster-2.etcd-headless:2379 endpoint health 2>/dev/null; do
            sleep 5
          done
          echo "etcd cluster is ready, starting aggressive data loading with etcdctl..."

          # Get pod name for unique keys
          POD_NAME=$(hostname)

          # Set etcdctl environment for v2 API
          export ETCDCTL_API=2

          # Aggressive data loading to fill up etcd and stress CSI volumes
          counter=1
          while true; do
            # Bulk write large entries (1MB each) to really fill up the volume
            for i in $(seq 1 10); do
              key="massive-data-${POD_NAME}-${counter}-${i}"
              # Create 1MB values to really stress the storage
              value=$(head -c 1048576 /dev/urandom | base64 -w 0)
              etcdctl --endpoints=http://etcd-cluster-$((counter % 3)).etcd-headless:2379 \
                set "${key}" "${value}" >/dev/null 2>&1 || true
            done

            # Write many smaller frequent updates (10KB each)
            for i in $(seq 1 50); do
              key="frequent-${POD_NAME}-${i}"
              value="timestamp-$(date +%s.%N)-counter-${counter}-$(head -c 10240 /dev/urandom | base64 -w 0)"
              etcdctl --endpoints=http://etcd-cluster-$((i % 3)).etcd-headless:2379 \
                set "${key}" "${value}" >/dev/null 2>&1 || true
            done

            # Write some huge entries (5MB each) to really push the limits
            for i in $(seq 1 3); do
              key="huge-data-${POD_NAME}-${counter}-${i}"
              value=$(head -c 5242880 /dev/urandom | base64 -w 0)
              etcdctl --endpoints=http://etcd-cluster-$((counter % 3)).etcd-headless:2379 \
                set "${key}" "${value}" >/dev/null 2>&1 || true
            done

            # Create many directories with nested data
            for dir in $(seq 1 5); do
              for file in $(seq 1 10); do
                key="stress-test/dir-${dir}/file-${file}-${POD_NAME}-${counter}"
                value="nested-data-$(date +%s.%N)-$(head -c 50240 /dev/urandom | base64 -w 0)"
                etcdctl --endpoints=http://etcd-cluster-$((dir % 3)).etcd-headless:2379 \
                  set "${key}" "${value}" >/dev/null 2>&1 || true
              done
            done

            counter=$((counter + 1))

            # Status report every 25 iterations
            if [ $((counter % 25)) -eq 0 ]; then
              echo "$(date): ${POD_NAME} completed ${counter} aggressive load cycles"
              # Try to get etcd member list and endpoint status
              etcdctl --endpoints=http://etcd-cluster-0.etcd-headless:2379 member list 2>/dev/null || true
            fi

            # Brief pause to prevent overwhelming the cluster
            sleep 1
          done
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "300m"